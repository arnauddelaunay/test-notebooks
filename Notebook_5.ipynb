{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"center\" width=\"300\" src=\"static/images/logo-training.png\" />\n",
    "\n",
    "<h1  style=\"text-align:center\"> Notebook #5 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Trees, Forests, and Democracy</h1>\n",
    "<p>\n",
    "    A new class of models will be featured in this notebook. But before we dive into it, lets sum up what we saw until now :\n",
    "    <ul>\n",
    "        <li>A Data Science project <em>always</em> starts with an <em>exploratory</em> phase</li>\n",
    "        <li>Given a target, we can produce simple models that will establish correlations between what we aim to predict and <em>features</em> </li>\n",
    "        <li>Any <em>categorical</em> feature can be (and must be) turned into numerical values.</li>\n",
    "        <li> Also, new features can be created by combining other ones</li>\n",
    "        <li>A model can be used to perform classification or regression depending on whether the target is categorical (or binary as we saw it) or continuous (as it was the case in the third notebook)</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<h2>Classification Trees </h2>\n",
    "<p>\n",
    "    The following cells will introduce you a new type of models : the classification trees. They work by learning simple rules on the features that will cut the dataset into smaller subsets</p>\n",
    "    <img src=\"https://www.ibm.com/support/knowledgecenter/SS3RA7_15.0.0/com.ibm.spss.modeler.help/images/dectree.gif\" />\n",
    "<p>\n",
    "    The <em>parameters</em> of a such model are the rules of each node, and the <em>shape</em> of the tree. To classify a sample, the model will simply make it flow through each node until it reaches a leaf.\n",
    "</p>\n",
    "<p>\n",
    "    The learning process works by fiding <em>optimal</em> cuts. A cut is a split over a feature depending on a certain value : <code>price_first_item_purchased</code> > 125\\$ for exemple is a cut.\n",
    "    <br >\n",
    "    To find those cuts, the algorithm uses a mesure called the <em>Gini impurty</em> : it represents the probability of miss-classifying a sample based on a certain cut. The <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\">CART</a> algorithm aims the lowest impurty when building nodes. It looks for <em>purety</em> when performing splits. The model will always try to find the most discriminative characteristics over the features with regards to the target.The training process ends when the classification score is above a minimal threshold, or when the tree contains a certain amount of node, or reaches a maximal height.\n",
    "</p>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*gx6XsQ8X_KEs7R05.png\" />\n",
    "<p>\n",
    "    The main benefit of the classification trees is their explainability : each and every sample is classified based on simple and <em>human-readable</em> rules : no explicit probabilities or coefficients are involved (as opposed to Logistic and Linear Regressions).\n",
    "</p>\n",
    "<p>\n",
    "Lets build all the features we need and assemble them : numerical, categorical, and computed features.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We import the usuals packages and the model from sklearn \n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from IPython.display import Image  \n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data/customerLifetimeValue.csv\", sep=\";\")\n",
    "#We take the columns we need for our models and get the underlying matrix\n",
    "X_numeric = dataset[[\"price_first_item_purchased\",  \"since_birth_parsed_days\"]].copy()\n",
    "#We also take a categorical variable\n",
    "X_categorical = dataset[[\"Country\", \"gender\"]].copy()\n",
    "#and we create a new feature\n",
    "X_numeric = X_numeric.assign(priceByVisited_pages = X_numeric[\"price_first_item_purchased\"]/X_numeric[\"pages_visited\"])\n",
    "#We binarize the target, all value greater than a given revenue will become positive (1), other negative(0)\n",
    "Y = dataset[\"revenue\"].apply(lambda x: 0 if x <= 175 else 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The cell above loads the features. The cell below replaces missing values by \"unknown\" and produces a column for each possible categorical value (remember the notebook nÂ°4).\n",
    "</p> \n",
    "<p>\n",
    "    In the following cell, you can notice that one <code>LabelBinarizer</code> is needed per feature.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_categorical.fillna(\"unknown\", inplace=True)\n",
    "countryBinarizer = LabelBinarizer()\n",
    "genderBinarizer = LabelBinarizer()\n",
    "binCountries = countryBinarizer.fit_transform(X_categorical[\"Country\"])\n",
    "binGenders = genderBinarizer.fit_transform(X_categorical[\"gender\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    At this stage, the X variable contains all the features and the Y all the targets values (revenue >175\\$). We will use a tree classifier as a model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All features are concatenated to produce the final X variable\n",
    "X = np.hstack([X_numeric.values, binCountries, binGenders])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Above : splitting the dataset in test and train sets <br /> Below : training a model (note how the api is the same for every model).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_leaf_nodes=8)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Similarly, testing the model is as simple as :\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Yhat = model.predict(X_test)\n",
    "print(accuracy_score(Y_test, Yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    It's a very decent score considering the size of the dataset and the nature of the model. Now, lets see how we can interpret those results and better understand the model we've produced : the previous models we saw used coefficients associated to each feature. The model automatically established correlations and learned the propper parameters. Trees work differently.\n",
    "</p>\n",
    "<p>\n",
    "    A tree classifier will try to find the most relevant splits (or cuts). It looks for discriminative values in our features with regard to the target. The node of the tree can be displayed using <a href=\"http://webgraphviz.com/\">this link</a> : copy-paste the result of the execution of the following cell (it simply produces a .dot code format that describes the tree) :\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genderBinarizer.classes_[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "featuresNames = list(X_numeric.columns) + list(countryBinarizer.classes_) + list(genderBinarizer.classes_)[:1]\n",
    "dot_data = tree.export_graphviz(model ,out_file=\"./graph.dot\", class_names=[\"Low Revenue\", \"High Revenue\"], feature_names=featuresNames)\n",
    "! cat graph.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Now, an ultimate steps lays between you and your goal of becoming an aprentice data scientist :\n",
    "<br />\n",
    "    The data science <em>iteration</em> process is the following : exploration, feature engineering, testing. Now it is your turn to go through the whole cycle. Follow the following steps :\n",
    "    <ul>\n",
    "        <li>Use the <code>dataset[\"since_birth_parsed_days\"].hist()</code> function to draw the age distribution of the population</li>\n",
    "        <li>Based on your observations, build a new feature</li>\n",
    "        <li>Stack your new feature with the previously created <code>X</code> variable to create a new training set</li>\n",
    "        <li>Create and train a <code>RandomForestClassifier</code></li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    And before you start, a quick word about Random Forest Classifiers.\n",
    "</p>\n",
    "<h2>Random Forest Classifiers</h2>\n",
    "<p>\n",
    "    The model we've featured previously was seducing in many ways : it was simple, elegant, understandable, and had decent results. Knowing that this model is callled a <em>Tree</em>, you may now imagine what we designate as a <em>Forest</em>. Again the idea is really intuitive ; instead of using one tree classifier, we use an ensemble of tree classifiers and make them <em>vote</em>.\n",
    "</p>\n",
    "<img src=\"https://www.researchgate.net/profile/Evaldas_Vaiciukynas/publication/301638643/figure/fig1/AS:355471899807744@1461762513154/Architecture-of-the-random-forest-model.png\" />\n",
    "\n",
    "<p>\n",
    "    The intuition behind this new method is to leverage a well known problem of the trees. The simpliest way of improving the score of a tree is to make it more complex, more <em>convoluted</em>. This produces <em>overfitted</em> models and all explainability is lost. The tree becames over specialized in predicting on the test set and creates ultra-specific rules (you can observe this by re-training a more complex tree on the cells above and draw it). To avoid this, a <code>RandomForestClassifier</code> will train multiple simple trees over small subsets of the train set. This way, we avoid overfitting and we keep the single models simple. Sadly, the interpretability is lost as a sigle prediction is the result of a vote over sometimes more than a thousand trees.\n",
    "</p>\n",
    "<p>\n",
    "    Ok ! Now that you know what you're dealing with, you are ready to produce you're first very own model ! Good luck !\n",
    "  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "<h1 style=\"text-align:center; color:orange\">YOUR TURN</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3-4.3.0",
   "language": "python",
   "name": "anaconda3-4.3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
